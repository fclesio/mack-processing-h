{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Instanciamento do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apontamento dos caminhos da instalação do Spark\n",
    "import os\n",
    "import sys\n",
    "\n",
    "spark_path = \"/Users/flavio.clesio/Documents/spark-2.1.0\" \n",
    "\n",
    "os.environ['SPARK_HOME'] = spark_path\n",
    "os.environ['HADOOP_HOME'] = spark_path\n",
    "\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.10.4-src.zip\") # Deve ser o mesmo arquivo da versão do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Libs para o instanciamento do contexto do Spark e do Spark SQL\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup do contexto do Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Aula de SQL Spark no Python!\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuração do diretório padrão no qual os arquivos estão no disco\n",
    "ROOT_DIR = \"/Users/flavio.clesio/Desktop/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalhando com arquivos .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classe para trabalhar com linhas em um dataframe\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos chamar o contexto do Spark como `sc`\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Agora vamos fazer a carga do arquivo people.txt que está no caminho especificado acima\n",
    "lines = sc.textFile(ROOT_DIR + \"people.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Michael, 29', u'Andy, 30', u'Justin, 19']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No objeto parts vamos gravar via o uso da transformation 'map' todas as palavras do arquivo lines,\n",
    "# separando por vírgula.\n",
    "parts = lines.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Michael', u' 29'], [u'Andy', u' 30'], [u'Justin', u' 19']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Já no objeto people, vamos realizar um map no RDD parts que foi armazenado acima, e para cada linha vamos realizar\n",
    "# o apontamento via [chave, valor] chamando a posição 1 de nome e a segunda posição de idade. \n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos criar um schema chamado 'schemaPeople' em que vamos criar um dataframe com o RDD que foi carregado do txt\n",
    "# transformado, e armazenado no RDD people\n",
    "schemaPeople = spark.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaPeople.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Com esse schema instanciado, vamos criar uma vire chamada people\n",
    "schemaPeople.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jovens = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovens.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalhando com Schemas, Dataframes e RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos carregar a lib que faz a conversão de data types\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos instanciar o contexto do Spark mais uma vez\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10e215950>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Carga do arquivo txt\n",
    "lines = sc.textFile(ROOT_DIR + \"/people.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quebra das palavras usando o delimitador ','\n",
    "parts = lines.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nesse caso, a função lambda, realiza a transformação para tupla usando o método strip()\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Michael', u'29'), (u'Andy', u'30'), (u'Justin', u'19')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Com essa string, vamos colocar o nome das nossas colunas\n",
    "schemaString = \"name age\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Após isso, vamos declarar os nossos campos de acordo com a divisão do nosso schemaString\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(name,StringType,true), StructField(age,StringType,true)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aqui vamos definir a estrutura do nosso schema\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(age,StringType,true)))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos aplicar o Schema no RDD people declarado anteriormente\n",
    "schemaPeople = spark.createDataFrame(people, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos aplicar o Schema no RDD people declarado anteriormente\n",
    "schemaPeople.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Como já registramos a nossa o nosso arquivo people em um dataframe, estamos aptos a realizar a consulta nesse objeto\n",
    "resultado = spark.sql(\"SELECT name FROM people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# E no fim temos o nosso resultado\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escrita e carga de arquivos Parquet, e consulta em arquivos Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Primeiramente vamos realizar a carga do nosso arquivo people.jSON no objeto DF\n",
    "df = spark.read.load(ROOT_DIR + \"people.json\", format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se quisermos salvar esse arquivo, podemos selecionar o nome das colunas que queremos\n",
    "# e passar o formato no qual desejamos que o mesmo seja salvo\n",
    "df.select(\"name\", \"age\").write.save(ROOT_DIR + \"namesAndAges.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM parquet.`/Users/flavio.clesio/Desktop/namesAndAges.parquet`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|Michael|null|\n",
      "|   Andy|  30|\n",
      "| Justin|  19|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algumas consultas bacanas no Spark ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Carga do arquivo jSON\n",
    "df = spark.read.json(ROOT_DIR + \"people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exibir o Schema e as propriedades dos campos\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleção de colunas\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleção de colunas\n",
    "df.select(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age * 5)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|      150|\n",
      "| Justin|       95|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Operações aritméticas\n",
    "df.select(df['name'], df['age']*5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Operador condicional\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|   name|sum(age)|\n",
      "+-------+--------+\n",
      "|Michael|    null|\n",
      "|   Andy|      30|\n",
      "| Justin|      19|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"age\").sum(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios de fixação\n",
    "###### (Aqui nóis constrói QUERY!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(ROOT_DIR + \"brasileiro.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+--------------------+---+---+---+------+-----+---+---+---+---+-----+---+\n",
      "|  %| 1º| 2º| 3º| 4º|  D|  E|             Equipes| GC| GP|  J|Melhor|Média|Pos|Pts|  R| SG|Temp.|  V|\n",
      "+---+---+---+---+---+---+---+--------------------+---+---+---+------+-----+---+---+---+---+-----+---+\n",
      "| 56|  3|  1|  3|  2|151|138|           São Paulo|606|845|552|     1|   66|  1|927|  0|239|   14|263|\n",
      "| 53|  3|  1|  1|  1|182|113|            Cruzeiro|699|870|552|     1|   63|  2|884|  0|171|   14|257|\n",
      "| 51|  1|  3|  0|  1|174|139|              Santos|681|860|552|     1|   61|  3|856|  0|179|   14|239|\n",
      "| 51|  0|  3|  1|  0|179|136|       Internacional|632|738|552|     2|   60|  4|849|  1|106|   14|237|\n",
      "| 52|  3|  0|  1|  1|146|146|         Corinthians|568|697|514|     1|   62|  5|814|  1|129|   13|222|\n",
      "| 48|  2|  0|  1|  1|189|146|          Fluminense|719|771|552|     1|   57|  6|799|  0| 52|   14|217|\n",
      "| 47|  1|  0|  2|  1|179|162|            Flamengo|693|722|552|     1|   56|  7|791|  0| 29|   14|211|\n",
      "| 50|  0|  2|  3|  1|168|128|              Grêmio|585|692|510|     2|   59|  8|770|  1|107|   13|214|\n",
      "| 48|  0|  2|  0|  1|177|134|    Atlético Mineiro|592|750|514|     2|   57|  9|743|  1| 58|   13|203|\n",
      "| 47|  0|  1|  1|  0|190|121| Atlético Paranaense|681|705|514|     2|   56| 10|730|  1| 24|   13|203|\n",
      "+---+---+---+---+---+---+---+--------------------+---+---+---+------+-----+---+---+---+---+-----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1)Quais foram os times que fizeram mais de 220 gols, e que jogaram mais de 10 temporadas?\n",
    "# 2)Quais foram os top 10 times que mais marcaram gols e que jogaram todos os 14 campeonatos?\n",
    "# 3)Quais foram os times que nunca ficaram em primeiro, segundo, terceiro e quarto lugares?\n",
    "# 4)Quais são os clubes que tem o maior saldo de gols negativos (Top 10)?\n",
    "# 5)Quais foram os times que ganharam o brasileirão mais de 2 vezes?\n",
    "# 6)Clubes que já foram rebaixados ordenados por quantidade de vezes que foram rebaixados\n",
    "# 7)Quais foram os times que nunca ficaram em primeiro lugar?\n",
    "# 8)Quais foram os times ficaram mais vezes em terceiro lugar?\n",
    "# 9)Quais foram os times que nunca ficaram em terceiro lugar?\n",
    "# 10)Quais são os clubes que tem a media maior de 60 gols\n",
    "# 11)Bottom 10 times que menos emparatam\n",
    "# 12)Quais foram os times ficaram mais vezes em primeiro lugar?\n",
    "# 13)Bottom 10 em aproveitamento\n",
    "# 14)Clubes que nunca foram rebaixados\n",
    "# 15)Tirando os clubes que tem mais de 10 temporadas, quais deles tem o maior número de gols?\n",
    "# 16)Quais foram os bomttom 10 times que mais marcaram gols e que jogaram todos os 14 campeonatos?\n",
    "# 17)Quais foram os times que nunca passaram da terceira posição\n",
    "# 18)Quais foram os times ficaram mais vezes em quarto lugar?\n",
    "# 19)Top 10 times que mais empataram\n",
    "# 20)Quais foram os times que nunca ficaram em segundo lugar?\n",
    "# 21)Quais são os times que tem mais de 50% de aproveitamento?\n",
    "# 22)Top 10 com melhores aproveitamentos em %\n",
    "# 23)Quais foram os times ficaram mais vezes em segundo lugar?\n",
    "# 24)Quais foram os times que nunca ficaram em quarto lugar?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
